{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Natural Language Content Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- **Le**xical analysis (POS tagging)\n",
    "- **S**yntactic analysis\n",
    "- **S**emantic analysis\n",
    "- **P**ragmatic analysis\n",
    "- **D**iscourse analysis\n",
    "\n",
    "**LeSSPD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical analysis (POS tagging)\n",
    "\n",
    "Also known as *lexical analysis*.\n",
    "\n",
    "Determine the basic units of a sentence and the meaning of each unit.\n",
    "\n",
    "This entails labeling each word in a sentence according to its syntactic category, e.g., \n",
    "- noun phrase\n",
    "- verb phrase\n",
    "- prepositional phrase etc.\n",
    "\n",
    "This gives the *structure of the sentence*, but not its meaning.\n",
    "\n",
    "## Syntactic analysis\n",
    "\n",
    "This step determines the relationships between the words in a sentence, and hence reveals the syntactic structure of a sentence.\n",
    "\n",
    "\n",
    "## Semantic analysis (parsing)\n",
    "\n",
    "This step uses the meaning of words and the syntactic structure from the previous two steps to determine the meaning of a sentence or a larger linguistic unit.\n",
    "\n",
    "## Pragmatic analysis\n",
    "\n",
    "This step determines the meaning in context, for e.g., to determine the reason(s) behind the actions described in a sentence. Put differently, the goal here is to understand the purpose in communication.\n",
    "\n",
    "## Discourse analysis\n",
    "\n",
    "Analyze a large chunk of text, for e.g., a set of sentences, by taking into account the connections between the sentences and interpreting the meaning of each sentence in context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges (WAPA)\n",
    "\n",
    "Ambiguities arise because natural language is not designed for computers.\n",
    "\n",
    "Also, computer lacks background knowledge to help it disambiguate text.\n",
    "\n",
    "\n",
    "## Word level ambiguities\n",
    "\n",
    "Some words are overloaded with different meaning (**ambiguous sense**)\n",
    "and/or belong to different syntactic categories (**ambiguous POS**).\n",
    "\n",
    "For e.g., the word \"design\" can be either a noun or a verb.\n",
    "\n",
    "\n",
    "## Syntactic ambiguities\n",
    "\n",
    "A phrase or sentence may have multiple valid syntactic structures, each leading to a different meaning.\n",
    "\n",
    "### Ambiguous modification\n",
    "\n",
    "For e.g., \"Natural language processing\" can mean \"processing of natural language\" or \"natural processing of language\".\n",
    "\n",
    "It is unclear if the word \"natural\" modifies \"language\" or \"processing\".\n",
    "\n",
    "### Prepositional phrase attachment ambiguities\n",
    "\n",
    "Also known as *PP attachment ambiguities*.\n",
    "\n",
    "E.g., \"A man saw a boy with a telescope\". It is unclear who has the telescope. Is it the man who had used it to see the boy? Or did it belong to the boy and the man saw the boy carrying it?\n",
    "\n",
    "This ambiguity arose because it is unclear which entity the phrase \"with the\" is associated with.\n",
    "\n",
    "\n",
    "## Anaphora resolution\n",
    "\n",
    "This ambiguity arise due to uncertainty about which entity a pronoun refers to. In the sentence \"John persuaded Bill to buy a television for himself\", it is unclear if \"himself\" refers to \"John\" or \"Bill\".\n",
    "\n",
    "## Presupposition\n",
    "\n",
    "The sentence \"He has quit smoking\" implies that he has smoked before. It is difficult for a computer to make this inference in general.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the art in NLP\n",
    "\n",
    "Deep understanding is still hard too achieve, especially in the general sense. High accuracy is restricted to specific domains/datasets.\n",
    "\n",
    "Can achieve fairly high accuracy (with usual caveats) for \n",
    "\n",
    "- POS tagging \n",
    "- Partial parsing.\n",
    "- Entity relation extraction\n",
    "- Word sense disambiguation\n",
    "- Sentiment analysis\n",
    "\n",
    "Deep semantic meaning is still hard to achieve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for text retrieval\n",
    "\n",
    "- Must be general & efficient -> Shallow retrieval.\n",
    "- \"Bag of words\" representation sufficient for most search tasks because some tasks require fairly \"crude\" results - as long as the document contains the queried words it is likely to be relevant.\n",
    "\n",
    "- Some text retrieval techniques can address NLP problems. For e.g., retrieving documents with words in a query can achieve word sense disambiguation because some specific meaning of a word only occur frequently with some other words.\n",
    "\n",
    "- Complex search tasks still require deep NLP (e.g., machine translation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text access modes\n",
    "\n",
    "## Push vs Pull\n",
    "\n",
    "Who initiates the access?\n",
    "\n",
    "Pull (think search engine **pulls** in user): User initiates by requesting for ad-hoc information.\n",
    "\n",
    "Push (think recommenders **push** content to user): System recommends information to user based on some knowledge about user.\n",
    "\n",
    "\n",
    "## Querying vs Browsing\n",
    "\n",
    "Depends on which mode is **convenient** for user. \n",
    "\n",
    "Convenience depends on whether you know the search terms or if it is easy to express the search terms or enter the query.\n",
    "\n",
    "- **Query**: If you know what you are looking for.\n",
    "- **Browsing**: If you want to explore data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is text retrieval\n",
    "\n",
    "User query -> System -> Relevant documents.\n",
    "\n",
    "## TR vs Database Query\n",
    "\n",
    "**Information**\n",
    "- Unstructured vs structured.\n",
    "- Ambiguous vs well-defined semantics.\n",
    "\n",
    "**Query**\n",
    "- Ambiguous vs well-defined semantics.\n",
    "- Incomplete vs complete specification.\n",
    "\n",
    "\n",
    "**Answers**\n",
    "- Relevant documents vs matched records.\n",
    "\n",
    "**TR is empirically defined problem**\n",
    "- Cannot mathematically prove one method is better than another.\n",
    "- Must rely on empirical evaluations involving users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formal formulation of TR\n",
    "\n",
    "**Vocabulary**: $V = \\{w_1, w_2, \\ldots, w_N\\}$ of language.\n",
    "\n",
    "**Query**: $q = q_1, \\ldots, q_m$ where $q_i \\in V$.\n",
    "\n",
    "**Document**: $d_i = d_{i1}, \\ldots, d_{im_i}$ where $d_{ij} \\in V$.\n",
    "\n",
    "**Collection**: $\\mathcal{C} = \\{d_1, \\ldots, d_M \\}$.\n",
    "\n",
    "**Relevant documents**: $\\mathcal{R(q)} \\subseteq \\mathcal{C} $.\n",
    "\n",
    "**Task**: Compute $\\mathcal{R^\\prime}(q) \\approx \\mathcal{R}(q)$.\n",
    "\n",
    "\n",
    "## Computing $\\mathcal{R^\\prime}(q)$\n",
    "\n",
    "1. Document selection\n",
    "\n",
    "System determines if document is relevant. Absolute relevant. No ranking.\n",
    "\n",
    "2. Document ranking\n",
    "\n",
    "System determines if document is more relevant than another (relative relevance). \n",
    "\n",
    "User determines cutoff.\n",
    "\n",
    "- Ranking in general is preferred because it is slightly \"easier\" because the classifier that performs document selection is unlikely perfectly accurate or might even return no relevant documents because the query might be overly specific (**\"over-constrained\"** query).\n",
    "\n",
    "- In other cases, the query might be too ambiguous (**\"under-constrained\"** query), leading to too many results. Without ranking, it is difficult for the user to navigate the result.\n",
    "\n",
    "- Hard to find the right position between these two extremes because user is not clear what he/she is looking for or knows the correct way to express the query.\n",
    "\n",
    "## \n",
    "\n",
    "- **Ranking function**: $f(q, d) \\in \\mathbb{R}$.\n",
    "   - Should rank relevant documents above irrelevant ones.\n",
    "   - Challenge is how to measure likelihood that document $d$ is relevant to query $q$.\n",
    "\n",
    "- **Retrieval models** = formalization of relevance (gives a computational definition to relevance).\n",
    "\n",
    "## Different retrieval model\n",
    "\n",
    "- **Similarity-based model**: $f(q, d)$ = similarity(q, d).\n",
    "- **Probabilistic model**: $f(q, d)$ = $P(R = 1 \\vert q, d)$ where $R \\in \\{0, 1\\}$.\n",
    "  - Classic probabilistic model\n",
    "  - Language model\n",
    "  - Divergence-from-random model\n",
    "- **Probabilistic inference model**: $f(q, d) = P(d \\rightarrow q)$.\n",
    "- **Axiomatic model**: $f(q, d)$ must satisfy a set of constraints.\n",
    "\n",
    "These models tend to result in similar ranking functions involving similar variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common ideas in TR\n",
    "\n",
    "- Term frequency (how often term appears in document).\n",
    "- Document length (term appearing in shorter document might be more relevant).\n",
    "- Document frequency (how often term appears in entire collection).\n",
    "\n",
    "\n",
    "# Popular retrieval models\n",
    "\n",
    "When optimized, the following models work equally well.\n",
    "\n",
    "- BM25\n",
    "- Pivoted length normalization\n",
    "- Query likelihood\n",
    "- PL2\n",
    "\n",
    "**BM25** is most popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space model (VSM)\n",
    "\n",
    "- Represent documents and queries as a length $N$ vector.\n",
    "- Relevance = similarity between $q$ and $d$.\n",
    "- Each dimension of vector should ideally represent orthogonal concepts.\n",
    "- Each entry in vector is a weight of the corresponding concept.\n",
    "\n",
    "## Unspecified part of VSM \n",
    "\n",
    "- Does not say how to define/select the basic \"concept\".\n",
    "- How to assign term weights for docs and query.\n",
    "- How to define the similarity measure.\n",
    "\n",
    "\n",
    "## Vector placement: Bit vector\n",
    "\n",
    "- $q = (x_1, \\ldots, x_N), d = (y_1, \\ldots, y_N)$\n",
    "   - $x_i, y_i \\in \\{0, 1\\}$. 1: word $w_i$ is present, else 0.\n",
    "   \n",
    "## Dot product as similarity\n",
    "\n",
    "- $f(q, d) = q \\cdot d$.\n",
    "- For bit vector case, this is simply the number of distinct terms that match between the two. This gives equal weight to all terms.\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
