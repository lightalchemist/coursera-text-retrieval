{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Metrics can be transformed in whatever way we want as long as it does not become biased towards any particular system, i.e., the order of performance is preserved.\n",
    "\n",
    "### Precision\n",
    "\n",
    "\\begin{equation}\n",
    "P = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Recall\n",
    "\n",
    "\\begin{equation}\n",
    "R = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### F1\n",
    "\n",
    "\\begin{equation}\n",
    "F_\\beta = \\frac{1}{ \\frac{\\beta^2}{\\beta^2 + 1}\\frac{1}{P} +  \\frac{1}{\\beta^2 + 1}\\frac{1}{R}}\n",
    "\\end{equation}\n",
    "\n",
    "This is the harmonic mean weighted by the parameter $\\beta \\in (0, 1)$\n",
    "\n",
    "\\begin{equation}\n",
    "F_1 = \\frac{2PR}{P + R}\n",
    "\\end{equation}\n",
    "\n",
    "is the special case for $\\beta = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating ranked list\n",
    "\n",
    "### Precision-Recall (PR) Curve\n",
    "\n",
    "Given ranked list, go from top to bottom and compute the corresponding precision and recall $(P_i, R_i)$ at each position $i$.\n",
    "\n",
    "Plot these coordinates with $P$ as y-axis and $R$ as x-axis. This is known as PR curve.\n",
    "\n",
    "Assume precision outside for items outside the list has precision 0.\n",
    "\n",
    "Area under the PR curve is a measure of the quality of the algorithm that generated this list.\n",
    "\n",
    "### Average Precision (AP)\n",
    "\n",
    "The average of precision at every cutoff where a new relevant document is retrieved.\n",
    "\n",
    "Normalizer = total number of relevant documents in collection.\n",
    "\n",
    "Sensitive to the rank of each relevant document.\n",
    "\n",
    "Compute precision and recall $(P_i, R_i)$ at each position $i$.\n",
    "From top rank to bottom of list, average the precision corresponding to a change in recall.\n",
    "\n",
    "In this example, assume there are **10 relevant documents** in the collection.\n",
    "\n",
    "| Doc  | Precision | Recall |\n",
    "|------|-----------|--------|\n",
    "| D1 + | **1/1**   | 1/10   |\n",
    "| D2 + | **2/2**   | 2/10   |\n",
    "| D3 - | 2/3       |        |\n",
    "| D4 - | 2/4       |        |\n",
    "| D5 + | **3/5**   | 3/10   |\n",
    "| D6 - |           |        |\n",
    "| D7 - |           |        |\n",
    "| D8 + | **4/8**   | 4/10   |\n",
    "| D9 - | 4/9       |        |\n",
    "| D10 -| 4/10      |        |\n",
    "\n",
    "\\begin{equation}\n",
    "AP = \\frac{ \\frac{1}{1} + \\frac{2}{2} + \\frac{3}{5} + \\frac{4}{8} + \\overbrace{0 + \\ldots + 0}^{\\text{Entries where recall stagnants}} }{ \n",
    "\\underbrace{10}_{\\text{# of relevant documents in collection}} }\n",
    "\\end{equation}\n",
    "\n",
    "For e.g., if two adjacent points have the same recall, it means they either are on top of each other (when they have the same precision) or one is below the other (when one precision is smaller than the other). \n",
    "By only considering precision for points where there is a change in recall, we ignore the points \"below the curve\".\n",
    "\n",
    "Special case when there is only **one known** relevant document: \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{AP} = \\text{Reciprocal Rank} = \\frac{1}{r}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Mean Average Precision (MAP)\n",
    "\n",
    "Mean of average precision over **a set of queries**.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MAP} := \\frac{\\sum_{i=1}^N \\text{AP}_i}{N}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Geometric mean average precision (gMAP)\n",
    "\n",
    "Geometric mean of average precisions over a set of queries.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{gMAP} := \\left( \\prod_{i=1}^N \\text{AP}_i \\right)^{\\frac{1}{N}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-level relevance judgement\n",
    "\n",
    "The previous measures only take into account binary relevance (i.e., relevant or not relevant).\n",
    "\n",
    "They are not applicable if we want to assign a \"score\"/\"weight\" to represent the degree of relevance of each retrieved document.\n",
    "\n",
    "E.g., relevance level: r = 1 (non-relevant), 2 (marginally-relevant), 3 (very relevant).\n",
    "\n",
    "The gain/relevance is a measure of the \"utility\" of a document to the user.\n",
    "\n",
    "### Discounted cumulative gain (DCG)\n",
    "\n",
    "We need to **discount** the gain based on the position of the document in the list so that **relevant documents with higher rank** is better than those below.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{DCG@k} := \\text{rel}_1 + \\sum_{i = 2}^k \\frac{ \\overbrace{\\text{rel}_i}^{\\text{Relevance for ith doc}} }\n",
    "{ \\underbrace{\\log_2 i}_{\\text{Discount factor for position i}} }\n",
    "\\end{equation}\n",
    "\n",
    "**NOTE** that we can find variations of how NDCG is computed online.\n",
    "\n",
    "\n",
    "### Normalized discounted cumulative gain (NDCG)\n",
    "\n",
    "It is not possible to compare DCG across different query and collections because the DCG can have different scales.\n",
    "By normalizing the DCG, we make all queries contribute equally to an aggregated score.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{NDCG@k} := \\frac{\\text{DCG@k}}{\\text{Ideal DCG@k}}\n",
    "\\end{equation}\n",
    "\n",
    "where Ideal DCG@k is the best possible DCG@k score for that collection and query.\n",
    "This maps the DCG score to the range (0, 1).\n",
    "\n",
    "Here is an example.\n",
    "\n",
    "| Doc | Gain / Relvance | Cumulative Gain | DCG        |\n",
    "|-----|------|----------------------------|------------|\n",
    "|D1   | 3    | 3 | 3                                   |\n",
    "|D2   | 2    | 5 | $3 + \\frac{2}{\\log_2 2}$            | \n",
    "|D3   | 1    | 6 | $3 + \\frac{2}{\\log_2 2} + \\frac{1}{\\log_2 3}$            | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance testing\n",
    "\n",
    "For a given set of queries and the performance for a number of systems, we want to\n",
    "use statistical testing to judge if score is statistically signficant or due to random chance\n",
    "when we determine if a system is better than another.\n",
    "\n",
    "E.g., a simple sign test (system A > or < B) along with p-value to test if A is better than B.\n",
    "\n",
    "Another is Wilcoxon test, which takes into account the difference between the scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling to create test collection\n",
    "\n",
    "Naturally we want to minimize the amount of work needed to create a test collection.\n",
    "\n",
    "We want to select a subset of documents to judge.\n",
    "\n",
    "Pooling strategy works by choosing a diverse set of systems and have each return the top-K documents\n",
    "for human assessors to judge.\n",
    "\n",
    "Documents not judged assumed to be irrelevant (though they don't have to be).\n",
    "\n",
    "Valid for comparing systems that contributed to the pool but problematic for those that did not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
