{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector space model - Improved instantiation\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "- Improve quality of weight of term by downweighting frequently occurring (hence less informative) terms.\n",
    "- $x_i = c(w_i, d) \\cdot \\text{IDF}(w_i) $ where $c(w_i, d)$ is count of word $w_i$ in document $d$.\n",
    "\n",
    "\n",
    "\n",
    "## IDF weighting: Penalize popular terms\n",
    "\n",
    "$\\text{IDF}(w) = \\log\\left( \\frac{M + 1}{k} \\right)$\n",
    "\n",
    "- $M$ is the total number of documents in the collection \n",
    "- $k$ is total number of documents containing $w$ (doc frequency).\n",
    "- Max value at $\\log\\left(M + 1\\right)$.\n",
    "- Min value at $\\log\\left(\\frac{M + 1}{M}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF transformation\n",
    "\n",
    "- Originally, $f(q, d) = \\sum_{w \\in q \\cap d} c(w, q) \\cdot c(w, d) \\log \\left(\\frac{M + 1}{df(w)} \\right)$.\n",
    "\n",
    "- Each occurrence of $w$ has the same weight. \n",
    "  - Undesirable because if $w$ appears frequently in $d$, any additional times it occur does not provide much more information than before.\n",
    "- Want to adjust the contribution based on the count.\n",
    "\n",
    "\n",
    "### BM25\n",
    "\n",
    "- $y = \\frac{(k + 1)x}{x + k}$ where $x = c(w, d)$ and $y$ is the new weight.\n",
    "- $k$ is a parameter to vary.\n",
    "- Different values of $k$ corresponds to different model.\n",
    "  - $k = 0$ is bit vector.\n",
    "- Upperbounds/limits weight of term. \n",
    "  - Limits influence of individual terms. Prevents spammer from spamming individual terms and overrun the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document length\n",
    "\n",
    "\n",
    "\n",
    "## Document length normalization\n",
    "\n",
    "- Penalize long document\n",
    "  - Longer document more likely to match any query.\n",
    "  - But need to avoid over-penalization.\n",
    "- Long document because\n",
    "  - uses more words $\\implies$ more penalization.\n",
    "  - more content $\\implies$ less penalization.\n",
    "  \n",
    "- Pivoted length normalizer: average doc length as \"pivot\".\n",
    "  - Normalizer = 1 if $\\vert d \\vert = $ average doc length ($\\text{avdl}$).\n",
    "  - Normalizer = $1 - b + b\\frac{\\vert d \\vert}{\\text{avdl}} $ where $b \\in [0, 1]$.\n",
    "  - $b = 0 \\implies $ normalizer = 1, so penalization.\n",
    "  - Larger $b \\implies $ more penalization.\n",
    "  \n",
    "# Well known models\n",
    "\n",
    "## Pivoted length normalization VSM (Singhal et al 96)\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f(q, d) = \\underbrace{ \\sum_{w \\in q \\cap d} }_{\\text{Sum over common terms}}\n",
    "\\underbrace{\n",
    "c(w, q)\n",
    "}_{\\text{Query word count} }\n",
    "\\underbrace{\n",
    "\\overbrace{\n",
    "\\frac{ \\overbrace{ \\ln \\left(1 + \\ln (1 + c(w, d)) \\right) }^{ \\text{Double log down-weighting of doc word count } }  }\n",
    "{ \n",
    "\\underbrace{1 - b + b \\frac{\\vert d \\vert }{\\text{avdl}}}_{\\text{Document length normalization} } }\n",
    "}^{\\text{TF transformation}}\n",
    "\\underbrace{\\log \\frac{M  + 1}{df(w)}}_{IDF(w)}\n",
    "}_{ \\text{Weighted document word count} }\n",
    "\\end{equation}\n",
    "\n",
    "This basically introduces a double logarithm transformation on the count and normalize it using document normalization.\n",
    "\n",
    "\n",
    "## BM25 / Okapi (Robertson et al)\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "f(q, d) = \\sum_{w \\in q \\cap d} c(w, q)\n",
    "\\underbrace{\n",
    "\\frac{(k + 1) c(w, d) }{c(w, d) + k \\left(1 - b + b \\frac{\\vert d \\vert }{\\text{avdl}} \\right)}\n",
    "}_{\\text{BM25 transformation } \\leq k + 1}\n",
    "\\log \\frac{M + 1}{df(w)}\n",
    "\\end{equation}\n",
    "\n",
    "This form is similar to the previous pivoted length normalization VSM except that the document term weighting is performed differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further improvements to VSM\n",
    "\n",
    "- Improve instantiation of dimension\n",
    "  - Stem words, stop word removal, phrases, latent semantic analysis (LSI), character n-grams...\n",
    "  - BOW with phrases is often sufficient.\n",
    "  - Language-specific and domain-specific tokenization is important to ensure \"normalization of terms\".\n",
    "  \n",
    "- Improved instantiation of similarity function\n",
    "  - Dot product with appropriate term weighting seems to still work the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical IR architecture\n",
    "\n",
    "- Tokenizer\n",
    "  - Normalize lexical units: Ensure words with similar meaning gets mapped to same indexing term.\n",
    "  - Stemming: Map all infectional forms to same root form. E.g., Computer, computation, computing -> compute.\n",
    "  \n",
    "- Indexer (offline)\n",
    "  - Convert documents to data structures to enable fast search (precompute as much as we can).\n",
    "  - Inverted index, document index etc.\n",
    "\n",
    "- Scorer (online)\n",
    "- Feedback (offline and online)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "   <img src=\"https://nlp.stanford.edu/IR-book/html/htmledition/img54.png\"/>\n",
    "  \n",
    "- Score can be computed easily from the statistics stored in the indexed without having to scan the documents on the fly.\n",
    "- Relevant documents can be retrieved quickly from the index.\n",
    "- Works because of word distributions in text - Zipf law.\n",
    "- A few words appear very frequently in text but most words are rare.\n",
    "- Frequent words in one corpus may be rare in another.\n",
    "\n",
    "### Zipf's law\n",
    "\n",
    "- rank * frequency of word $\\approx$ constant - $F(w) = \\frac{C}{r(w)^\\alpha}, \\alpha \\approx 1, C \\approx 0.1 $.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ac/Zipf_30wiki_en_labels.png\" width=500 height=500 />\n",
    "\n",
    "### Data structures\n",
    "\n",
    "- Dictionary\n",
    "  - Fast random access.\n",
    "  - Preferred to be in memory.\n",
    "  - Hash-table, B-Tree, trie, ...\n",
    "- Postings\n",
    "  - Sequential access is expected.\n",
    "  - Large, so store on disk.\n",
    "  - May contain docID, term freq, term pos, etc.\n",
    "  - Compress is desirable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index compression\n",
    "\n",
    "\n",
    "### Compressing word count\n",
    "\n",
    "A form of variable length encoding where we use few bits too encode small numbers and more bits to encode large ones. Due to phenomenon behind Zipf's law, most words tend to appear infrequently, so their word counts and the number of document they appear in will tend to be small integers. Encoding them this way allows us to save memory.\n",
    "\n",
    "### Compressing document ID\n",
    "\n",
    "Furthermore, for document ID (docID), in the inverted index, we don't store the docIDs of the document containing a word. Instead, we sort the docID, store the first one, and then store the adjacent differences (known as \"d-gap\"). This way, we keep the numbers small, and they can in turn also be compressed efficiently using the scheme above.\n",
    "\n",
    "## Integer compression\n",
    "\n",
    "We can reduce memory by compressing the integers stored in the index using variable length encoding.\n",
    "\n",
    "There are a few common encoding schemes,\n",
    "\n",
    "1. Binary\n",
    "2. Unary\n",
    "3. Gamma\n",
    "4. Delta\n",
    "\n",
    "### Binary\n",
    "\n",
    "This is just the binary string representation of the integer without any 0s as prefix.\n",
    "\n",
    "### Unary\n",
    "\n",
    "$x \\ge 1$ is coded as $x - 1$ one bits followed by a 0. \n",
    "\n",
    "This is aggressive in compressing small numbers but disastrous when we encounter a large number.\n",
    "\n",
    "### Gamma ($\\gamma$) code\n",
    "\n",
    "This is a less aggressive form of unary coding.\n",
    "\n",
    "To encode $x$, \n",
    "\n",
    "Unary code $\\underbrace{ 1 + \\lfloor \\log x \\rfloor }_{k = \\text{ # of bits in binary representation of } x} $ then concatenate with\n",
    "binary code of \n",
    "$ \\underbrace{ x - \n",
    "\\underbrace{2^{ \\lfloor \\log x \\rfloor }}_{\\text{Largest 2 power } \\leq x}}_{\n",
    "P }$ using $k$ bits (padding with 0s if required).\n",
    "\n",
    "Note that $\\lfloor \\log x \\rfloor$ is exactly the number of 1s in the unary portion because the unary portion is coded as $1 + \\lfloor \\log x \\rfloor - 1 = \\lfloor \\log x \\rfloor$ number of 1s followed by a 0.\n",
    "\n",
    "To decode, compute $x = 2^{\\lfloor \\log x \\rfloor } + P $.\n",
    "\n",
    "**Note**: All the above operations can be computed very easily and efficiently using bit-level operations.\n",
    "\n",
    "An alternative given in the textbook (and IMO better unless $P$ can be compressed efficiently) is to store $k - 1$ 0s followed by a 1 and then followed by the binary representation of $x$. The 1 act as a delimiter and the number of 0s $+ 1$ tells us how many bits to read after that to get the binary representation of $x$.\n",
    "\n",
    "\n",
    "### Delta ($\\delta$) code\n",
    "\n",
    "Same as $\\gamma$ code but replace unary coding part with $\\gamma$ code.\n",
    "\n",
    "This is a less aggressive version of $\\gamma$ code.\n",
    "\n",
    "\n",
    "<font color=\"red\">Note:</font> that we cannot code for 0 under all these schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast search using inverted index\n",
    "\n",
    "- We can use inverted index to accumulate scores for documents matching query terms.\n",
    "- We need to first retrieve the docID their corresponding statistics of documents matching the terms in the query.\n",
    "- Then, allocate one accumulator (variable) for each document to store its accumulated score.\n",
    "- After all the scores have been accumulated, we can return the scores and docID.\n",
    "\n",
    "## Why use inverted index\n",
    "- Using inverted index to look up document actually allows us to exploit Zipf's law phenomenon to retrieve information about a relatively small set of documents.\n",
    "- Storing the appropriate set of statistics about the terms and documents can allow us to quickly compute the scores for a wide range of ranking algorithms.\n",
    "- Scaling up requires use of distributed file system and parallel processing and caching."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
