{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Feedback in IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical language model\n",
    "\n",
    "- A distribution over word sequences.\n",
    "- A generative distribution for sequences of words.\n",
    "\n",
    "### Unigram language model (LM)\n",
    "\n",
    "Assumes words in a sentence are generated **independently**.\n",
    "\n",
    "\\begin{align}\n",
    "P(w_1, \\ldots, w_n \\mid \\theta) &= \\prod_{i=1}^n P(w_i \\mid \\theta)\\\\ \n",
    "\\sum_{i=1}^n P(w_i \\mid \\theta) &= 1\n",
    "\\end{align}\n",
    "\n",
    "#### Maximum likelihood (ML) estimator\n",
    "\n",
    "The following is the probability of generating a word $w$ based on a unigram model learned from a document $d$.\n",
    "\\begin{align}\n",
    "P(w \\mid \\theta) = P(w \\mid d) = \\frac{c(w, d)}{\\mid d \\mid}\n",
    "\\end{align}\n",
    "\n",
    "#### Background LM\n",
    "\n",
    "A model for language for a \"background\" topic. Basically a language model trained on a general collection of document rather than documents for a specific topic.\n",
    "\n",
    "\\begin{equation}\n",
    "P(w \\mid B)\n",
    "\\end{equation}\n",
    "\n",
    "#### Collection LM\n",
    "\n",
    "\\begin{equation}\n",
    "P(w \\mid C)\n",
    "\\end{equation}\n",
    "\n",
    "#### Document LM\n",
    "\n",
    "\\begin{equation}\n",
    "P(w \\mid d)\n",
    "\\end{equation}\n",
    "\n",
    "### Maximum-likelihood estimation\n",
    "\n",
    "We can estimate the above probability distribution using maximum-likelihood (ML) estimation simply by finding the fraction of words in the respective cases that matches a particular word $w$. E.g.,\n",
    "\n",
    "\\begin{align}\n",
    "P(w \\mid d) = \\frac{c(w, d)}{\\lvert d \\rvert}\n",
    "\\end{align}\n",
    "\n",
    "### Normalized topic model\n",
    "\n",
    "We can use the background language model to down-weigh words that are uninformative for a given topic. \n",
    "For e.g., words that appear frequently in **any** given context will have a high probability based on the background LM but this is not true for topic-specific words.\n",
    "\n",
    "One way to down-weigh uninformative words is to divide the probability of each word from a document or collection LM by the probability of that word given by the background LM.\n",
    "\n",
    "\\begin{equation}\n",
    " P_{\\text{normalized}}(w) = \\frac{P(w \\mid d)}{P(w \\mid C)}\n",
    "\\end{equation} \n",
    " \n",
    "or if we just need to compare the relative magnitude, we can compute the log-likelihood,\n",
    "\n",
    "\\begin{equation}\n",
    " \\log P_{\\text{normalized}}(w) = \\log P(w \\mid d) - \\log P(w \\mid C)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Unigram query likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "P(q = (w_1, \\ldots, w_N) \\mid d) = \\prod_{i=1}^N P(w_i \\mid d)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## Improved model: sampling words from a document model\n",
    "\n",
    "If we assume that words in a query must be sampled from existing documents, then queries containing words that are not found in a document will have 0 probability of being generated from that document under the indepent sampling assumption.\n",
    "\n",
    "A potential fix is to assume that there is a hypothetical document that exists in the user's mind and this document is represented by a document model that is to be estimated and compute the necessary probabilities from this model.\n",
    "\n",
    "\n",
    "## Scoring function for ranking documents based on query likelihood\n",
    "\n",
    "\\begin{align}\n",
    "\\text{For } q &= (w_1, \\ldots, w_n) , \\\\ \n",
    " f(q, d) & = \\log P(q \\mid d) \\\\\n",
    " &= \n",
    "\\underbrace{\\sum_{i=1}^n}_{ \\substack{ \\text{Sum over words} \\\\ \\text{in query} } } \n",
    "\\log P(w_i \\mid d) \\\\\n",
    " & =   \n",
    "\\underbrace{ \\sum_{ w \\in \\mathcal{V} } }\n",
    "_{ \\substack{ \\text{Sum over words} \\\\ \\text{in vocabulary} } } \n",
    "c(w, q) \\log P(w \\mid d)\n",
    "\\end{align}\n",
    "\n",
    "The goal of retrieval is to estimate $P(w \\mid d)$.\n",
    "\n",
    "Different models for $P(w \\mid d)$ gives different ranking models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating $P(w \\mid d)$\n",
    "\n",
    "Need to have a way to assign probability to unseen word so that they do not end up with 0 probability.\n",
    "\n",
    "Let probability of unseen word be proportional to a reference language model (usually collection language model).\n",
    "\n",
    "\\begin{align}\n",
    "P(w \\mid d) = \n",
    "\\begin{cases}\n",
    "&P_{\\text{Seen}}(w \\mid d) &, & \\text{if } w \\text{ is in } d\\\\\n",
    "&\n",
    "\\alpha_d \\underbrace{P(w \\mid C)}_{ \\substack{ \\text{Background probability} \\\\ \\text{as reference LM} } } &, \n",
    "& \\text{ otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Note that **we still need to estimate $P_{\\text{Seen}}(w \\mid d)$ and how to set $\\alpha_d$.** This will be explained below.\n",
    "\n",
    "For now, we can substitute the expression for $P(w \\mid d)$ into the expression for $f(q, d)$\n",
    "split the sum according to the two cases of $P(w \\mid d)$ \n",
    "to obtain a smoothed ranking function.\n",
    "\n",
    "\n",
    "## Ranking function with smoothing\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\log P(q \\mid d) \n",
    "& = \n",
    "\\overbrace { \n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} \n",
    "c(w, q) \\log P_{\\text{Seen}}(w \\mid d) \n",
    "}^{\\text{Contribution from words in doc}} \n",
    "&+& \n",
    "\\overbrace {\n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) = 0} \n",
    "c(w, q) \\log \\alpha_d P(w \\mid C)\n",
    "}^{ \\text{Contribution from words } \\textbf{not } \\text{in doc} } \\\\\n",
    "& =\n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} \n",
    "c(w, q) \\log P_{\\text{Seen}}(w \\mid d) \n",
    "&+& \n",
    "\\sum_{w \\in \\mathcal{V}} c(w, q) \\log \\alpha_d P(w \\mid C) -\n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} c(w, q) \n",
    "\\log \\alpha_d P(w \\mid C) \\\\\n",
    "& = \n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} \n",
    "c(w, q) \\log \\frac{P_{\\text{Seen}}(w \\mid d)}{\\alpha_d P(w \\mid C)}\n",
    "&+& \n",
    "\\sum_{w \\in \\mathcal{V}} c(w, q) \\log \\alpha_d P(w \\mid C) \\\\\n",
    "& = \n",
    "\\underbrace {\n",
    "\\sum_{w \\in q \\cap d} \n",
    "}\n",
    "_{ \\substack{ \\text{Common words} \\\\ \\text{in query and doc} } }\n",
    "c(w, q) \\log \n",
    "\\frac{\n",
    "   \\overbrace{P_{\\text{Seen}}(w \\mid d)}^{\\text{TF weighting}}\n",
    "}\n",
    "{ \n",
    "  \\underbrace{ \\alpha_d P(w \\mid C) }_{\\text{IDF weighting}}\n",
    "}\n",
    "&+& \n",
    "\\underbrace { \n",
    "n\\log \\alpha_d \n",
    "}_{ \\substack{ \\text{Document normalization.} \\\\\n",
    "\\text{Precompute } \\because \\\\ \\text{ independent of query}\n",
    "} }\n",
    "+\n",
    "\\underbrace{\n",
    "\\sum_{w \\in \\mathcal{V}} c(w, q) \\log P(w \\mid C)\n",
    "}_{ \\substack{ \\text{Ignore } \\because \\\\ \\text{independent of doc} } }.\n",
    "\\end{align}\n",
    "\n",
    "We then ignore the sum that is independent of the document and define the scoring function to be\n",
    "\n",
    "\\begin{align}\n",
    "f(q, d) \n",
    "&= \n",
    "\\sum_{w \\in q \\cap d} c(w, q) \\log \\frac{P_{\\text{Seen}}(w \\mid d)}{\\alpha_d P(w \\mid C)}\n",
    "+\n",
    "n \\log \\alpha_d.\n",
    "\\end{align}\n",
    "\n",
    "The two advantages to this approach to smoothing the function are\n",
    "\n",
    "1. **Efficient computation** because we only need to sum over the number of terms proportional to the size of the query.\n",
    "2. **Help us better understand TF-IDF weighting** by highlighting the similarities between the two.\n",
    "\n",
    "In particular, the expressions are obtained from a principled approach by stating the probabilistic modeling assumptions up front and the intuitive properties may not necessarily appear if we simply design the ranking function  based on heuristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate $P_{\\text{Seen}}(w \\mid d)$ and $\\alpha_d$\n",
    "\n",
    "### Linear interpolation (Jelinek-Mercer) smoothing\n",
    "\n",
    "One way we can smooth the seen word distribution by linearly interpolating the ML-estimated model with the background model\n",
    "\n",
    "\\begin{align}\n",
    "P_{\\text{Seen}}(w \\mid d) \n",
    "&= (1 - \\lambda) P_{\\text{ML}}(w \\mid d) + \\lambda P(w \\mid C), \\ \\lambda \\in [0, 1]\\\\\n",
    "&= (1 - \\lambda) \\frac{c(w, d)}{\\lvert d \\rvert} + \\lambda P(w \\mid C)\n",
    "\\end{align}\n",
    "\n",
    "This ensures $P_{\\text{Seen}}(w \\mid d)$ dooes not return 0 probabilities for words not in the document $d$.\n",
    "\n",
    "### Dirichlet (Bayesian) smoothing\n",
    "\n",
    "Another way we can smooth $P_{\\text{Seen}}(w \\mid d)$ is given $\\mu \\in [0, \\infty)$, let\n",
    "\n",
    "\\begin{align}\n",
    "P_{\\text{Seen}}(w \\mid d) \n",
    "&= \\frac{ P_{\\text{ML}}(w \\mid d) + \\mu P(w \\mid C) }\n",
    "{\\lvert d \\rvert + \\mu} \\\\\n",
    "&= \n",
    "\\underbrace{ \\frac{ \\lvert d \\rvert }{\\lvert d \\rvert + \\mu} }\n",
    "_{ \\substack{ \\text{Doc length dependent} \\\\ \\text{interpolating weights}} }\n",
    "\\frac{c(w, d)}{\\lvert d \\rvert} + \n",
    "\\underbrace {\n",
    "\\frac{\\mu}{\\lvert d \\rvert + \\mu} \n",
    "}_{ \\substack{\\text{For fixed } \\mu \\\\ \\text{longer doc } \\\\ \\rightarrow \\text{ less weight} }}\n",
    "P(w \\mid C) \\\\\n",
    "&= \\frac{c(w, d) + \n",
    "\\overbrace {\n",
    "\\mu P(w \\mid C) }^{\\text{Pseudo-word counts}} }\n",
    "{ \\lvert d \\rvert + \\underbrace{\\mu}_{ \\substack{ \\text{ Total pseudo} \\\\ \\text{-word counts}} } }\n",
    "\\end{align}\n",
    "\n",
    "Similar to Jelinek-Mercer smoothing, \n",
    "the second expression \n",
    "above tells us that we are also doing a linear interpolation between $P_{\\text{ML}}(w \\mid d)$\n",
    "and the collection background model $P(w \\mid C)$,\n",
    "but in this case the weights are dependent on the length of each document, $\\lvert d \\rvert$, \n",
    "and the parameter $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine exact form of ranking function\n",
    "\n",
    "We now need to determine $\\alpha_d$ for each of the smoothing methods.\n",
    "\n",
    "### JM smoothing\n",
    "\n",
    "First, we substitute $P_{\\text{Seen}}(w \\mid d)$ into the ratio in the expression of $f(q, d)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{P_{\\text{Seen}}(w \\mid d)}{\\alpha_d P(w \\mid C)}\n",
    "&= \n",
    "\\frac {(1 - \\lambda) P_{\\text{ML}}(w \\mid d) + \\lambda P(w \\mid C)}\n",
    "      {\\lambda P(w \\mid C))} \\\\\n",
    "&= \n",
    "1 + \\frac{1 - \\lambda }{\\lambda} \\frac{c(w, d)}{\\lvert d \\rvert P(w \\mid C)}, \\text{ where } \\lambda \\in [0, 1].\n",
    "\\end{align}\n",
    "\n",
    "Plugging this into the expression for $f(q, d)$, we get\n",
    "\\begin{align}\n",
    "f(q, d) \n",
    "&= \n",
    "\\sum_{w \\in q \\cap d} c(w, q) \n",
    "\\log \\left( 1 + \\frac{1 - \\lambda }{\\lambda} \\frac{c(w, d)}{\\lvert d \\rvert P(w \\mid C)} \\right)\n",
    "+\n",
    "n \\log \\alpha_d,\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha_d = \\lambda$.\n",
    "\n",
    "Because the term $n \\log \\alpha_d = n \\log \\lambda$ is independent of the document, we can ignore it and obtain\n",
    "the ranking function\n",
    "\n",
    "\\begin{align}\n",
    "f(q, d) \n",
    "&= \n",
    "\\sum_{w \\in q \\cap d} c(w, q) \n",
    "\\log \n",
    "\\left( \n",
    "1 + \n",
    "\\frac{1 - \\lambda }{\\lambda} \n",
    "\\overbrace {\n",
    "\\frac{c(w, d)}{ \\underbrace{ \\lvert d \\rvert P(w \\mid C) }_{\\text{Expected count of } w} } \n",
    "}^{\\text{Ratio of actual vs expected count}}\n",
    "\\right).\n",
    "\\end{align}\n",
    "\n",
    "which is a vector space model as it is just the dot product between the query vector and a weighted document vector.\n",
    "\n",
    "### Dirichlet smoothing\n",
    "\n",
    "\\begin{align}\n",
    "f(q, d) \n",
    "&= \n",
    "\\sum_{w \\in q \\cap d} c(w, q) \n",
    "\\log \n",
    "\\left( \n",
    "\\frac{ \\frac{ c(w, d) + \\mu P(w \\mid C) }\n",
    "            { \\lvert d \\rvert + \\mu     } }\n",
    "     { \\frac{\\mu P(w \\mid C) }{\\lvert d \\rvert + \\mu } } \n",
    "\\right) \n",
    "& + &  \n",
    "n \\log \\alpha_d, \\text{ where } \\alpha_d = \\frac{\\mu}{\\lvert d \\rvert + \\mu} \\\\\n",
    "&=   \n",
    "\\sum_{w \\in q \\cap d} c(w, q) \n",
    "\\log \n",
    "\\left( \n",
    "\\frac{ c(w, d) + \\mu P(w \\mid C) }\n",
    "     { \\mu P(w \\mid C)           } \n",
    "\\right) \n",
    "& + &  n \\log \\alpha_d \\\\\n",
    "&= \n",
    "\\sum_{w \\in q \\cap d} c(w, q) \n",
    "\\log \n",
    "\\left(\n",
    "1 +\n",
    "\\frac{ c(w, d) }\n",
    "     { \\mu P(w \\mid C) } \n",
    "\\right) \n",
    "& + &  n \\log \\alpha_d,\n",
    "\\end{align}\n",
    "\n",
    "where again we see a ratio of actual vs expected word counts in the $\\log$ function and the addition of 1 to prevent taking the logarithm of a 0."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
