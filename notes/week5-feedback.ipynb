{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of feedback\n",
    "\n",
    "1. Relevance feedback\n",
    "   - User indicates which of the returned search result are relevant and which are not.\n",
    "2. Pseudo feedback\n",
    "   - Assumes top K documents retrieved by the search engine are relevant.\n",
    "   - Obtain additional relevant words **not in query** from these top K documents, for e.g., using a background language model.\n",
    "   - These additional relevant words can be used to expand the original query.\n",
    "   - **No user interaction** is required beyond entering the query.\n",
    "3. Implicit\n",
    "   - Obtained user's preferences indirectly by inferring his/her behavior for e.g. from clickthrough logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback for vector space model\n",
    "\n",
    "### General method: query modification\n",
    "\n",
    "- Adjust query vector \n",
    "  - Add new (weighted) terms\n",
    "  - Adjust weights of old terms\n",
    "\n",
    "### Rocchio\n",
    "\n",
    "  - Adjust query vector \n",
    "    - to be more similar to some \"averaged vector\" of relevant documents and\n",
    "    - less similar to those irrelevant ones.\n",
    "\n",
    "Given a query vector $\\mathbf{q}$, the updated vector $\\mathbf{q_m}$ computed using weights\n",
    "$\\alpha, \\beta, \\gamma \\in \\mathbb{R_{+}}$ is\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{q_m} = \\alpha \n",
    "                \\underbrace{\\mathbf{q}}_{\\text{Original query}} \n",
    "                +\n",
    "                \\beta\n",
    "                \\underbrace {\n",
    "                \\sum_{ \\mathbf{d} \\in \\mathcal{D_r}}\n",
    "               \\frac{\\mathbf{d}}\n",
    "                    {\\lvert \n",
    "                     \\underbrace{\\mathcal{D_r}}_{ \\substack{ \\text{Relevant}\\\\\\text{docs} } }  \n",
    "                     \\rvert}\n",
    "                }_{ \\text{Average relevant doc} } -               \n",
    "               \\gamma\n",
    "               \\underbrace {\n",
    "                \\sum_{ \\mathbf{d} \\in \\mathcal{D_n}}\n",
    "               \\frac{\\mathbf{d}}\n",
    "                    {\\lvert \n",
    "                     \\underbrace{\\mathcal{D_n}}_{ \\substack{ \\text{Irrelevant}\\\\\\text{docs} } }  \n",
    "                     \\rvert}\n",
    "               }_{ \\text{Average irrelevant doc} }\n",
    "\\end{align}\n",
    "\n",
    "### In practice\n",
    "\n",
    "1. Irrelevant centroid usually less important since it represents a large range of topics and hence will drag the query vector all over the place.\n",
    "2. Often truncate vector to consider only elements of centroid vectors with highest weights.\n",
    "3. Keep \"relative\" high weights of original query vector since the terms are entered by the user and hence should be precise.\n",
    "4. $\\beta$ should be higher for relevant feedback than for pseudo feedback as in the former case we know that the relevant documents are accurate because the user marked them as so whereas in the latter case we only assume that they are.\n",
    "5. Usually robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback in language model approaches\n",
    "\n",
    "**Problem:** Query likelihood cannot naturally support relevance feedback.\n",
    "\n",
    "**Solution**:\n",
    "- Kullback-Leibler (KL) divergence retrieval model as a generalization of query likelihood \n",
    "- Feedback is achieved through query model estimation/updating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler (KL) Divergence Retrieval Model\n",
    "\n",
    "Query likelihood model\n",
    "\\begin{align}\n",
    "f(q, d) \n",
    "&= \n",
    "\\sum_{w \\in q \\cap d} c(w, q) \\log \\frac{P_{\\text{Seen}}(w \\mid d)}{\\alpha_d P(w \\mid C)}\n",
    "+\n",
    "n \\log \\alpha_d.\n",
    "\\end{align}\n",
    "\n",
    "KL-divergence model\n",
    "\\begin{align}\n",
    "f(q, d)\n",
    "&=\n",
    "\\sum_{w \\in d, P(w \\mid \\hat{\\theta}_Q) > 0 } \n",
    "\\underbrace{\n",
    "P(w \\mid \\hat{\\theta}_{Q}) \n",
    "}_{\\substack{\\text{For query LM, } \\\\ \\text{set to } \\frac{c(w, q)}{\\lvert q \\rvert} }}\n",
    "\\log \\frac{P_{\\text{Seen}}(w \\mid d)}{\\alpha_d P(w \\mid C)}\n",
    "+\n",
    "n \\log \\alpha_d.\n",
    "\\end{align}\n",
    "\n",
    "So the goal is to estimate $P(w \\mid \\hat{\\theta}_{Q})$. One way to incorporate feedback in estimating this model is to first obtain a set of feedback documents that are assumed to be relevant $\\mathcal{F} = \\{d_i\\}_{i=1}^n$ and then estimate $P(w \\mid \\hat{\\theta}_{Q})$ as the distribution that is most likely to have generated this $\\mathcal{F}$.\n",
    "\n",
    "To do this, we first model $P(w \\mid \\hat{\\theta}_{Q})$ as the mixture of a background and topic model\n",
    "\n",
    "\\begin{align}\n",
    "P(\\mathcal{F} \\mid \\theta) &= \n",
    "\\prod_{i=1}^n \\prod_{w \\in d_i}\n",
    "\\left( (1 - \\lambda) \\underbrace{P(w \\mid \\theta)}_{\\text{Topic model}} + \n",
    "\\lambda \\underbrace{P(w \\mid C)}_{ \\substack{\\text{Collection} \\\\ \\text{topic model}}} \\right)\n",
    "^{ \\overbrace{c\\left(w, d_i\\right)}^{\\text{Count of } w \\text{ in } d_i} } \n",
    "\\text{ where } \\lambda \\in [0, 1]\n",
    "\\end{align}\n",
    "and we seek $\\theta^\\ast$ such that\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^\\ast \n",
    "&= \\underset{\\theta}{\\arg \\max} \\log P(\\mathcal{F} \\mid \\theta) \\\\\n",
    "&= \\underset{\\theta}{\\arg \\max} \\sum_{i=1}^n \\sum_{w \\in d_i} c(w, d_i) \n",
    "                                \\log \\left( (1 - \\lambda) P(w \\mid \\theta) + \n",
    "                                \\lambda P(w \\mid C) \\right)\n",
    "\\end{align}\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
