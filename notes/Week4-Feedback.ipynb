{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Feedback in IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical language model\n",
    "\n",
    "- A distribution over word sequences.\n",
    "- A generative distribution for sequences of words.\n",
    "\n",
    "### Unigram language model (LM)\n",
    "\n",
    "Assumes words in a sentence are generated **independently**.\n",
    "\n",
    "\\begin{align}\n",
    "P(w_1, \\ldots, w_n \\mid \\theta) &= \\prod_{i=1}^n P(w_i \\mid \\theta)\\\\ \n",
    "\\sum_{i=1}^n P(w_i \\mid \\theta) &= 1\n",
    "\\end{align}\n",
    "\n",
    "#### Maximum likelihood (ML) estimator\n",
    "\n",
    "The following is the probability of generating a word $w$ based on a unigram model learned from a document $d$.\n",
    "\\begin{align}\n",
    "P(w \\mid \\theta) = P(w \\mid d) = \\frac{c(w, d)}{\\mid d \\mid}\n",
    "\\end{align}\n",
    "\n",
    "#### Background LM\n",
    "\n",
    "A model for language for a \"background\" topic. Basically a language model trained on a general collection of document rather than documents for a specific topic.\n",
    "\n",
    "\\begin{equation}\n",
    "P(w \\mid B)\n",
    "\\end{equation}\n",
    "\n",
    "#### Collection LM\n",
    "\n",
    "\\begin{equation}\n",
    "P(w \\mid C)\n",
    "\\end{equation}\n",
    "\n",
    "#### Document LM\n",
    "\n",
    "\\begin{equation}\n",
    "P(w \\mid d)\n",
    "\\end{equation}\n",
    "\n",
    "### Normalized topic model\n",
    "\n",
    "We can use the background language model to down-weigh words that are uninformative for a given topic. \n",
    "For e.g., words that appear frequently in **any** given context will have a high probability based on the background LM but this is not true for topic-specific words.\n",
    "\n",
    "One way to down-weigh uninformative words is to divide the probability of each word from a document or collection LM by the probability of that word given by the background LM.\n",
    "\n",
    "\\begin{equation}\n",
    " P_{\\text{normalized}}(w) = \\frac{P(w \\mid d)}{P(w \\mid C)}\n",
    "\\end{equation} \n",
    " \n",
    "or if we just need to compare the relative magnitude, we can compute the log-likelihood,\n",
    "\n",
    "\\begin{equation}\n",
    " \\log P_{\\text{normalized}}(w) = \\log P(w \\mid d) - \\log P(w \\mid C)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Unigram query likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "P(q = (w_1, \\ldots, w_N) \\mid d) = \\prod_{i=1}^N P(w_i \\mid d)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## Improved model: sampling words from a document model\n",
    "\n",
    "If we assume that words in a query must be sampled from existing documents, then queries containing words that are not found in a document will have 0 probability of being generated from that document under the indepent sampling assumption.\n",
    "\n",
    "A potential fix is to assume that there is a hypothetical document that exists in the user's mind and this document is represented by a document model that is to be estimated.\n",
    "\n",
    "\n",
    "## Scoring function for ranking documents based on query likelihood\n",
    "\n",
    "\\begin{align}\n",
    "\\text{For } q = (w_1, \\ldots, w_n) , \\ \n",
    " f(q, d) & = \\log P(q \\mid d) \\\\\n",
    " &= \n",
    "\\underbrace{\\sum_{i=1}^n}_{ \\substack{ \\text{Sum over words} \\\\ \\text{in query} } } \n",
    "\\log P(w_i \\mid d) \\\\\n",
    " & =   \n",
    "\\underbrace{ \\sum_{ w \\in \\mathcal{V} } }\n",
    "_{ \\substack{ \\text{Sum over words} \\\\ \\text{in vocabulary} } } \n",
    "c(w, q) \\log P(w \\mid d)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating $P(w \\mid d)$\n",
    "\n",
    "Need to smooth the language model so that it does not produce 0 probability for words that are in the query but not seen in the document.\n",
    "\n",
    "\\begin{align}\n",
    "P(w \\mid d) = \n",
    "\\begin{cases}\n",
    "&P_{\\text{Seen}}(w \\mid d) &, & \\text{if } w \\text{ is in } d\\\\\n",
    "&\\underbrace{\n",
    "\\alpha_d P(w \\mid C)}_{ \\text{Background probability} } &, \n",
    "& \\text{ otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "## Ranking function with smoothing\n",
    "\n",
    "\\begin{align}\n",
    "\\log P(q \\mid d) \n",
    "& = \n",
    "\\overbrace { \n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} \n",
    "c(w, q) \\log P_{\\text{Seen}}(w \\mid d) \n",
    "}^{\\text{Contribution from words in doc}} \n",
    "&+& \n",
    "\\overbrace {\n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) = 0} \n",
    "c(w, q) \\log \\alpha_d P(w \\mid C)\n",
    "}^{ \\text{Contribution from words } \\textbf{not } \\text{in doc} } \\\\\n",
    "& =\n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} \n",
    "c(w, q) \\log P_{\\text{Seen}}(w \\mid d) \n",
    "&+& \n",
    "\\sum_{w \\in \\mathcal{V}} c(w, q) \\log \\alpha_d P(w \\mid C) -\n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} c(w, q) \n",
    "\\log \\alpha_d P(w \\mid C) \\\\\n",
    "& = \n",
    "\\sum_{w \\in \\mathcal{V}, c(w, d) > 0} \n",
    "c(w, q) \\log \\frac{P_{\\text{Seen}}(w \\mid d)}{\\alpha_d P(w \\mid C)}\n",
    "&+& \n",
    "\\sum_{w \\in \\mathcal{V}} c(w, q) \\log \\alpha_d P(w \\mid C) \\\\\n",
    "& = \n",
    "\\underbrace {\n",
    "\\sum_{w \\in q \\cap d} \n",
    "}\n",
    "_{ \\text{Size of query} }\n",
    "c(w, q) \\log \n",
    "\\frac{\n",
    "   \\overbrace{P_{\\text{Seen}}(w \\mid d)}^{\\text{TF weighting}}\n",
    "}\n",
    "{ \n",
    "  \\underbrace{ \\alpha_d P(w \\mid C) }_{\\text{IDF weighting}}\n",
    "}\n",
    "&+& \n",
    "\\underbrace { \n",
    "n\\log \\alpha_d \n",
    "}_{ \\substack{ \\text{Document normalization.} \\\\\n",
    "\\text{Precompute } \\because \\\\ \\text{ independent of query}\n",
    "                } }\n",
    "+\n",
    "\\underbrace{\n",
    "\\sum_{w \\in \\mathcal{V}} c(w, q) \\log P(w \\mid C)\n",
    "}_{ \\substack{ \\text{Ignore } \\because \\\\ \\text{independent of doc} } }\n",
    "\\end{align}\n",
    "\n",
    "There are two advantages to this are\n",
    "\n",
    "1. **Efficient computation** because we only need to sum over the number of terms proportional to the size of the query.\n",
    "2. **Help us better understand TF-IDF weighting** by recognizing the similarities between the two.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
